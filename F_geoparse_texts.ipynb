{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geoparsing\n",
    "\n",
    "[![colab badge](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mcallaghan/NLP-climate-science-tutorial-CCAI/blob/main/F_geoparse_texts.ipynb)\n",
    "\n",
    "The last thing we want to do with our texts is to geoparse them. This involves two steps: extracting place names and resolving these to the structured geographic information. The [Mordecai](https://github.com/openeventdata/mordecai) library does this, with the help of some neural networks to resolve combined_place_df names to the correct combined_place_df based on the context. You will want to install mordecai in a separate virtual environment - make sure this environment is using the latest version of pip, to make sure tensorflow gets installed correctly. Some people have had issues running Mordecai on Macs - in case this is not working, the output of this file is included.\n",
    "\n",
    "First we will load the data and merge it with the predictions. We only want to run the parser on documents predicted to be relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15636, 16)\n",
      "(14442, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>seen</th>\n",
       "      <th>INCLUDE</th>\n",
       "      <th>12 - Coastal and marine Ecosystems</th>\n",
       "      <th>12 - Human and managed</th>\n",
       "      <th>12 - Mountains, snow and ice</th>\n",
       "      <th>12 - Rivers, lakes, and soil moisture</th>\n",
       "      <th>12 - Terrestrial ES</th>\n",
       "      <th>title_lcase</th>\n",
       "      <th>OA_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>authors</th>\n",
       "      <th>INCLUDE_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2018832642</td>\n",
       "      <td>The analysis of possible regional climate chan...</td>\n",
       "      <td>An inter-comparison of regional climate models...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anintercomparisonofregionalclimatemodelsforeur...</td>\n",
       "      <td>https://openalex.org/W2018832642</td>\n",
       "      <td>https://doi.org/10.1007/s10584-006-9213-4</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Daniela Jacob, Lars Bärring, Ole Bøssing Chris...</td>\n",
       "      <td>0.515179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468699.0</td>\n",
       "      <td>The processes influencing the magnitude of Wes...</td>\n",
       "      <td>Climatic Controls on West Nile Virus and Sindb...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>climaticcontrolsonwestnilevirusandsindbisvirus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1284550.0</td>\n",
       "      <td>The long-term history of fire regimes in the M...</td>\n",
       "      <td>Coupled human-climate signals on the fire hist...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>coupledhumanclimatesignalsonthefirehistoryofup...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>695403.0</td>\n",
       "      <td>Since the late 1940s, snowmelt and runoff have...</td>\n",
       "      <td>LARGE-SCALE ATMOSPHERIC FORCING OF RECENT TREN...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>largescaleatmosphericforcingofrecenttrendstowa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1464308.0</td>\n",
       "      <td>Exploring the relationship between hydrologica...</td>\n",
       "      <td>Non-linear relationship of hydrological drough...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nonlinearrelationshipofhydrologicaldroughtresp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0   https://openalex.org/W2018832642   \n",
       "2                           468699.0   \n",
       "7                          1284550.0   \n",
       "10                          695403.0   \n",
       "11                         1464308.0   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   The analysis of possible regional climate chan...   \n",
       "2   The processes influencing the magnitude of Wes...   \n",
       "7   The long-term history of fire regimes in the M...   \n",
       "10  Since the late 1940s, snowmelt and runoff have...   \n",
       "11  Exploring the relationship between hydrologica...   \n",
       "\n",
       "                                                title  seen  INCLUDE  \\\n",
       "0   An inter-comparison of regional climate models...   0.0      NaN   \n",
       "2   Climatic Controls on West Nile Virus and Sindb...   1.0      1.0   \n",
       "7   Coupled human-climate signals on the fire hist...   1.0      1.0   \n",
       "10  LARGE-SCALE ATMOSPHERIC FORCING OF RECENT TREN...   1.0      1.0   \n",
       "11  Non-linear relationship of hydrological drough...   1.0      1.0   \n",
       "\n",
       "    12 - Coastal and marine Ecosystems  12 - Human and managed  \\\n",
       "0                                  NaN                     NaN   \n",
       "2                                  0.0                     1.0   \n",
       "7                                  0.0                     0.0   \n",
       "10                                 0.0                     0.0   \n",
       "11                                 0.0                     0.0   \n",
       "\n",
       "    12 - Mountains, snow and ice  12 - Rivers, lakes, and soil moisture  \\\n",
       "0                            NaN                                    NaN   \n",
       "2                            0.0                                    0.0   \n",
       "7                            0.0                                    0.0   \n",
       "10                           0.0                                    1.0   \n",
       "11                           0.0                                    1.0   \n",
       "\n",
       "    12 - Terrestrial ES                                        title_lcase  \\\n",
       "0                   NaN  anintercomparisonofregionalclimatemodelsforeur...   \n",
       "2                   0.0  climaticcontrolsonwestnilevirusandsindbisvirus...   \n",
       "7                   1.0  coupledhumanclimatesignalsonthefirehistoryofup...   \n",
       "10                  0.0  largescaleatmosphericforcingofrecenttrendstowa...   \n",
       "11                  0.0  nonlinearrelationshipofhydrologicaldroughtresp...   \n",
       "\n",
       "                               OA_id  \\\n",
       "0   https://openalex.org/W2018832642   \n",
       "2                                NaN   \n",
       "7                                NaN   \n",
       "10                               NaN   \n",
       "11                               NaN   \n",
       "\n",
       "                                          doi  publication_year  \\\n",
       "0   https://doi.org/10.1007/s10584-006-9213-4            2007.0   \n",
       "2                                         NaN               NaN   \n",
       "7                                         NaN               NaN   \n",
       "10                                        NaN               NaN   \n",
       "11                                        NaN               NaN   \n",
       "\n",
       "                                              authors  INCLUDE_prediction  \n",
       "0   Daniela Jacob, Lars Bärring, Ole Bøssing Chris...            0.515179  \n",
       "2                                                 NaN            1.000000  \n",
       "7                                                 NaN            1.000000  \n",
       "10                                                NaN            1.000000  \n",
       "11                                                NaN            1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "## If we are running in colab, mount google drive and change into the directory we cloned the repository into\n",
    "if os.path.exists(\"/content/\"):\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir(\"/content/drive/MyDrive/NLP-climate-science-tutorial-CCAI\") \n",
    "\n",
    "from D_run_cv_experiments import load_data\n",
    "\n",
    "df = load_data(False)\n",
    "df.loc[pd.isna(df[\"id\"]), \"id\"] = df.loc[pd.isna(df[\"id\"]), \"OA_id\"]\n",
    "\n",
    "# Merge data with predictions\n",
    "df = df.merge(pd.read_csv('cv_data/INCLUDE/predictions_5_splits.csv'), how=\"outer\")\n",
    "# Where we have no prediction, put the actual label in the prediction column.\n",
    "df.loc[pd.isna(df[\"INCLUDE_prediction\"]),\"INCLUDE_prediction\"] = df.loc[pd.isna(df[\"INCLUDE_prediction\"]),\"INCLUDE\"]\n",
    "print(df.shape)\n",
    "df = df[(df[\"INCLUDE_prediction\"]>=0.5)]\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also load any combined_place_df that have already been processed, or initialise an empty dataframe\n",
    "if os.path.exists(\"data/combined_place_df.csv\"):\n",
    "    processed_place_df = pd.read_csv(\"data/combined_place_df.csv\")\n",
    "    unprocessed_place_df = df[~df['id'].isin(processed_place_df)]\n",
    "else:\n",
    "    processed_place_df = pd.DataFrame()\n",
    "    unprocessed_place_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 10:54:40.148846: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-16 10:54:40.148869: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models path: /home/max/software/mordecai-env/lib/python3.9/site-packages/mordecai/models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 10:55:15.653547: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-16 10:55:15.653775: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-16 10:55:15.654179: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (max-ThinkPad-X280): /proc/driver/nvidia/version does not exist\n",
      "2022-08-16 10:55:15.655853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'Oxford',\n",
       "  'spans': [{'start': 17, 'end': 23}],\n",
       "  'country_predicted': 'GBR',\n",
       "  'country_conf': 0.95718795,\n",
       "  'geo': {'admin1': 'England',\n",
       "   'lat': '51.75222',\n",
       "   'lon': '-1.25596',\n",
       "   'country_code3': 'GBR',\n",
       "   'geonameid': '2640729',\n",
       "   'place_name': 'Oxford',\n",
       "   'feature_class': 'P',\n",
       "   'feature_code': 'PPLA2'}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When we run the geoparser on a string, we get nice structured geographical information\n",
    "from mordecai import Geoparser\n",
    "geo = Geoparser()\n",
    "geo.geoparse(\"I travelled from Oxford to Ottawa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "places = []\n",
    "geos = []\n",
    "\n",
    "import re\n",
    "\n",
    "# Go through the rows of the dataframe\n",
    "for i, row in unprocessed_place_df.iterrows():\n",
    "    \n",
    "    # Get the text we want to geoparse, join title and abstract, get rid of copyright stuff\n",
    "    t = str(row['title']) + \" \" + str(row['abstract'])\n",
    "    t = t.split(\"Copyright (C)\")[0] \n",
    "    t = re.split(\"\\([C-c]\\) [1-2][0-9]{3} Elsevier\",t)[0] \n",
    "    t = t.split(\"Published by Elsevier\")[0] \n",
    "    t = t.split(\"Copyright. (C)\")[0] \n",
    "    t = re.split(\"\\. \\(C\\) [1-2][0-9]{3} \",t)[0] \n",
    "    t = re.split(\"\\. \\(C\\) Copyright\",t)[0]   \n",
    "    t = re.split(\"\\. \\\\xA9 [1-2][0-9]{3}\", t)[0] #Copyright symbol\n",
    "    \n",
    "    # Remove some common place names involved in environmental studies\n",
    "    t = re.sub(\"paris agreement\", \"\", t, flags=re.I)\n",
    "    t = re.sub(\"kyoto protocol\", \"\", t, flags=re.I)\n",
    "    t = re.sub(\"montreal protocol\", \"\", t, flags=re.I)\n",
    "    t = re.sub(\"london protocol\", \"\", t, flags=re.I)\n",
    "    \n",
    "    # geoparse\n",
    "    gp = geo.geoparse(t)\n",
    "    \n",
    "    # For each place, append to a list of dictionaries, with a field for the doc_id\n",
    "    for p in gp:\n",
    "        if \"geo\" in p:\n",
    "            for key, value in p[\"geo\"].items():\n",
    "                p[key] = value\n",
    "            del p[\"geo\"]\n",
    "            \n",
    "        p[\"doc_id\"] = row[\"id\"]\n",
    "        places.append(p)\n",
    "\n",
    "    # Save this every thousand rows, so we don't need to start again if we get interrupted\n",
    "    if i % 1000 == 0:\n",
    "        combined_place_df = processed_place_df.append(pd.DataFrame.from_dict(places))\n",
    "        print(combined_place_df.shape)\n",
    "        combined_place_df.to_csv(\"data/combined_place_df.csv\", index=False)\n",
    "    \n",
    "# Merge all the data together\n",
    "combined_place_df = processed_place_df.append(pd.DataFrame.from_dict(places))\n",
    "print(combined_place_df.shape)\n",
    "combined_place_df.to_csv(\"data/combined_place_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning geoparsing output\n",
    "\n",
    "The output from Mordecai has some common errors. Some of the ones we have identified are fixed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12518/3168564461.py:60: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Paris(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Paris)')) |\n",
      "/tmp/ipykernel_12518/3168564461.py:61: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Paris(?:\\S* ){0,15}Agreement)|(COP(?:\\S* ){0,15}Agreement)')) ,\n",
      "/tmp/ipykernel_12518/3168564461.py:68: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Copenhagen(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Copenhagen)')) |\n",
      "/tmp/ipykernel_12518/3168564461.py:69: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Copenhagen(?:\\S* ){0,3}Accord)|(Accord(?:\\S* ){0,3}Copenhagen)')) ,\n",
      "/tmp/ipykernel_12518/3168564461.py:76: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Berlin(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Berlin)')),\n",
      "/tmp/ipykernel_12518/3168564461.py:83: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Glasgow(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Glasgow)')),\n",
      "/tmp/ipykernel_12518/3168564461.py:90: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (df['tstring'].str.contains('(Cancun(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Cancun)')) |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43064, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>spans</th>\n",
       "      <th>country_predicted</th>\n",
       "      <th>country_conf</th>\n",
       "      <th>admin1</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>country_code3</th>\n",
       "      <th>geonameid</th>\n",
       "      <th>place_name</th>\n",
       "      <th>feature_class</th>\n",
       "      <th>feature_code</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13756</th>\n",
       "      <td>Labrador Sea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>-55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3424929</td>\n",
       "      <td>Labrador Sea</td>\n",
       "      <td>H</td>\n",
       "      <td>SEA</td>\n",
       "      <td>https://openalex.org/W2066743080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14331</th>\n",
       "      <td>Labrador Sea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>-55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3424929</td>\n",
       "      <td>Labrador Sea</td>\n",
       "      <td>H</td>\n",
       "      <td>SEA</td>\n",
       "      <td>https://openalex.org/W1606968016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15441</th>\n",
       "      <td>Labrador Sea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>-55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3424929</td>\n",
       "      <td>Labrador Sea</td>\n",
       "      <td>H</td>\n",
       "      <td>SEA</td>\n",
       "      <td>https://openalex.org/W3171583560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11553</th>\n",
       "      <td>Baffin Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>-68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3831554</td>\n",
       "      <td>Baffin Bay</td>\n",
       "      <td>H</td>\n",
       "      <td>BAY</td>\n",
       "      <td>https://openalex.org/W2049963706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14331</th>\n",
       "      <td>Baffin Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>-68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3831554</td>\n",
       "      <td>Baffin Bay</td>\n",
       "      <td>H</td>\n",
       "      <td>BAY</td>\n",
       "      <td>https://openalex.org/W1606968016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word spans country_predicted  country_conf admin1 lat  lon  \\\n",
       "13756  Labrador Sea   NaN               NaN           0.8    NaN  57  -55   \n",
       "14331  Labrador Sea   NaN               NaN           0.8    NaN  57  -55   \n",
       "15441  Labrador Sea   NaN               NaN           0.8    NaN  57  -55   \n",
       "11553    Baffin Bay   NaN               NaN           0.8    NaN  74  -68   \n",
       "14331    Baffin Bay   NaN               NaN           0.8    NaN  74  -68   \n",
       "\n",
       "      country_code3 geonameid    place_name feature_class feature_code  \\\n",
       "13756           NaN   3424929  Labrador Sea             H          SEA   \n",
       "14331           NaN   3424929  Labrador Sea             H          SEA   \n",
       "15441           NaN   3424929  Labrador Sea             H          SEA   \n",
       "11553           NaN   3831554    Baffin Bay             H          BAY   \n",
       "14331           NaN   3831554    Baffin Bay             H          BAY   \n",
       "\n",
       "                                 doc_id  \n",
       "13756  https://openalex.org/W2066743080  \n",
       "14331  https://openalex.org/W1606968016  \n",
       "15441  https://openalex.org/W3171583560  \n",
       "11553  https://openalex.org/W2049963706  \n",
       "14331  https://openalex.org/W1606968016  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tstring'] = df['title'] + \" \" + df['abstract']\n",
    "\n",
    "gm_docs = df.loc[\n",
    "    (df['tstring'].str.lower().str.contains(\"gulf of mexico\")),\n",
    "    \"id\"\n",
    "]\n",
    "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
    "gm = pd.DataFrame({\"doc_id\": gm_docs})\n",
    "gm[geocolumns] = [\"Gulf of Mexico\",0.8,\"GULF\", 25, -90, \"Gulf of Mexico\", \"H\", 3523271]\n",
    "\n",
    "combined_place_df = pd.concat([combined_place_df, gm])\n",
    "\n",
    "\n",
    "lab_docs = df.loc[\n",
    "    (df['tstring'].str.lower().str.contains(\"labrador sea\")),\n",
    "    \"id\"\n",
    "]\n",
    "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
    "lab = pd.DataFrame({\"doc_id\": lab_docs})\n",
    "lab[geocolumns] = [\"Labrador Sea\",0.8,\"SEA\", 57, -55, \"Labrador Sea\", \"H\", 3424929]\n",
    "\n",
    "combined_place_df = pd.concat([combined_place_df, lab])\n",
    "\n",
    "baf_docs = df.loc[\n",
    "    (df['tstring'].str.lower().str.contains(\"baffin bay\")),\n",
    "    \"id\"\n",
    "]\n",
    "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
    "baf = pd.DataFrame({\"doc_id\": baf_docs})\n",
    "baf[geocolumns] = [\"Baffin Bay\",0.8,\"BAY\", 74, -68, \"Baffin Bay\", \"H\", 3831554]\n",
    "combined_place_df = pd.concat([combined_place_df, baf])\n",
    "\n",
    "\n",
    "ok_docs = df.loc[\n",
    "    (df['tstring'].str.lower().str.contains(\"sea of okhotsk\")) ,\n",
    "    \"id\"\n",
    "]\n",
    "geocolumns = [\"word\", \"country_conf\", \"feature_code\",\"lat\",\"lon\",\"place_name\",\"feature_class\",\"geonameid\"]\n",
    "ok = pd.DataFrame({\"doc_id\": ok_docs})\n",
    "ok[geocolumns] = [\"Sea of Okhotsk\",0.8, \"SEA\", 55, 150, \"Sea of Okhotsk\", \"H\", 2127380]\n",
    "combined_place_df = pd.concat([combined_place_df, ok])\n",
    "\n",
    "# Drop\n",
    "\n",
    "\n",
    "kyoto_docs = df.loc[\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto target\")) |\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto process\")) |\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto emission\")) |\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto gas\")) |\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto agreement\")) |\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto protocol\")) |\n",
    "    (df['tstring'].str.lower().str.contains(\"kyoto framework\")),\n",
    "    \"id\"\n",
    "]\n",
    "\n",
    "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(kyoto_docs)) & (combined_place_df['word'].str.lower()==\"kyoto\")].index)\n",
    "\n",
    "paris_docs = df.loc[\n",
    "    (df['tstring'].str.contains('(Paris(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Paris)')) |\n",
    "    (df['tstring'].str.contains('(Paris(?:\\S* ){0,15}Agreement)|(COP(?:\\S* ){0,15}Agreement)')) ,\n",
    "    'id'\n",
    "]\n",
    "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(paris_docs)) & (combined_place_df['word'].str.lower()==\"paris\")].index)\n",
    "\n",
    "# Copenhagen\n",
    "copenhagen_docs = df.loc[\n",
    "    (df['tstring'].str.contains('(Copenhagen(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Copenhagen)')) |\n",
    "    (df['tstring'].str.contains('(Copenhagen(?:\\S* ){0,3}Accord)|(Accord(?:\\S* ){0,3}Copenhagen)')) ,\n",
    "    'id'\n",
    "]\n",
    "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(copenhagen_docs)) & (combined_place_df['word'].str.lower()==\"copenhagen\")].index)\n",
    "\n",
    "#Berlin\n",
    "berlin_docs = df.loc[\n",
    "    (df['tstring'].str.contains('(Berlin(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Berlin)')),\n",
    "    'id'\n",
    "]\n",
    "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(berlin_docs)) & (combined_place_df['word'].str.lower()==\"berlin\")].index)\n",
    "\n",
    "#Glasgow\n",
    "berlin_docs = df.loc[\n",
    "    (df['tstring'].str.contains('(Glasgow(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Glasgow)')),\n",
    "    'id'\n",
    "]\n",
    "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(berlin_docs)) & (combined_place_df['word'].str.lower()==\"berlin\")].index)\n",
    "\n",
    "#Cancun\n",
    "cancun_docs = df.loc[\n",
    "    (df['tstring'].str.contains('(Cancun(?:\\S* ){0,15}COP)|(COP(?:\\S* ){0,15}Cancun)')) |\n",
    "    (df['tstring'].str.lower().str.contains('cancun pledge')),\n",
    "    'id'\n",
    "]\n",
    "combined_place_df = combined_place_df.drop(combined_place_df[(combined_place_df['doc_id'].isin(cancun_docs)) & (combined_place_df['word'].str.lower()==\"cancun\")].index)\n",
    "\n",
    "\n",
    "geocolumns = [\"feature_code\", \"lat\", \"lon\", \"place_name\", \"feature_class\", \"geonameid\", \"country_code3\"]\n",
    "\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Pakistan\", geocolumns]=[\"PCLI\",30,70,\"Islamic Republic of Pakistan\",\"A\",1168579,\"PAK\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Colombia\", geocolumns]=[\"PCLI\",4,-73.25,\"Colombia\",\"A\",3686110, \"COL\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Argentina\", geocolumns]=[\"PCLI\",-34,-64,\"Argentine Republic\",\"A\",3865483, \"ARG\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Sahara\", geocolumns] = [\"DSRT\", 26, 13, \"Sahara\", \"T\", 2212709, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Alps\",geocolumns] = [\"MTS\", 46.41667, 10, \"Alps\", \"T\", 2661786, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Mediterranean Sea\",geocolumns] = [\"SEA\", 35, 20, \"Mediterranean Sea\", \"T\", 2661786, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"MEDITERRANEAN\",geocolumns] = [\"SEA\", 35, 20, \"Mediterranean Sea\", \"T\", 2661786, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"East China\",geocolumns] = [\"PCLI\", 35, 105, \"China\", \"A\", 1814991, \"CHN\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"South China\",geocolumns] = [\"PCLI\", 35, 105, \"China\", \"A\", 1814991, \"CHN\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Great Lakes\",geocolumns] = [\"LK\", 45.68751, -84.43753, \"Great Lakes\", \"H\", 4994594, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Catalonia\",geocolumns] = [\"ADM1\", 41.82046, 1.86768, \"Catalunya\", \"A\", 3336901, \"ESP\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"South Pacific\",geocolumns] = [\"OCN\", -45, -130, \"South Pacific Ocean\", \"H\", 4030483, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Gulf Coast\",geocolumns] = [\"AREA\", 29.36901, -95.00565, \"Gulf Coast\", \"L\", 7287689, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Gulf coast\",geocolumns] = [\"AREA\", 29.36901, -95.00565, \"Gulf Coast\", \"L\", 7287689, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Hainan Island\",geocolumns] = [\"ISL\", 19.2, 109.7, \"Hainan Dao\", \"T\", 1809055, \"CHN\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Red Sea\",geocolumns] = [\"SEA\", 20.26735, 38.53455, \"Red Sea\", \"H\", 350155, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Himalayan\",geocolumns] = [\"MTS\", 28,84, \"Himalayas\", \"T\", 1252558, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Himalayas\",geocolumns] = [\"MTS\", 28,84, \"Himalayas\", \"T\", 1252558, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"North America's\",geocolumns] = [\"CONT\", 46.07323, -100.54688, \"North America\", \"L\", 6255149, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Atlantic Ocean\",geocolumns] = [\"OCN\", 10, -25, \"Atlantic Ocean\", \"H\", 3373405, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Scandinavia\",geocolumns] = [\"RGN\", 63, 12, \"Scandinavia\", \"L\", 2614165, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"California (USA\",geocolumns] = [\"ADM1\", 37.25022, -119.75126, \"California\", \"A\", 5332921, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"California, USA\",geocolumns] = [\"ADM1\", 37.25022, -119.75126, \"California\", \"A\", 5332921, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"North Pacific\",geocolumns] = [\"OCN\", 30, -170, \"North Pacific Ocean\", \"H\", 4030875, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Huai\",geocolumns] = [\"STM\", 33.133333, 118.5, \"Huai He\", \"H\", 1807690, \"CHN\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Washington, DC\",geocolumns] = [\"PPLC\", 38.89511, -77.03637, \"Washington\", \"P\", 4140963, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Messinian\",geocolumns] = [\"ADM2\", 37.25, -21.83333, \"Nomos Messinias\", \"A\", 257149, \"GRC\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Ionian Sea\",geocolumns] = [\"SEA\", 39, 19, \"Ionian Sea\", \"H\", 2463713, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"NYC\",geocolumns] = [\"PPL\", 40.71427, -74.00597, \"New York City\", \"P\", 5128581, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Indian Ocean\",geocolumns] = [\"OCN\", -10, 70, \"Indian Ocean\", \"P\", 1545739, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"North Sea\",geocolumns] = [\"SEA\", 55, 3, \"North Sea\", \"P\", 2960848, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Philippine Sea\",geocolumns] = [\"SEA\", 20, 135, \"Philippine Sea\", \"P\", 1818190, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Black Sea\",geocolumns] = [\"SEA\", 43, 34, \"Black Sea\", \"H\", 630673, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Coral Sea\",geocolumns] = [\"SEA\", -20, 155, \"Coral Sea\", \"H\", 2194166, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Timor Sea\",geocolumns] = [\"SEA\", -11, 127, \"Timor Sea\", \"H\", 2078065, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Hudson Bay\",geocolumns] = [\"BAY\", 60, -85, \"Hudson Bay\", \"H\", 5978134, \"CAN\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Bering Sea\",geocolumns] = [\"SEA\", 60, -175, \"Bering Sea\", \"H\", 4031788, None]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Okhotsk Sea\",geocolumns] = [\"SEA\", 55, 150, \"Sea of Okhotsk\", \"H\", 2127380, None]\n",
    "\n",
    "combined_place_df.loc[combined_place_df[\"place_name\"]==\"Central Upper Nile\",geocolumns] = [\"ADM1\", 10, 32.7, \"Upper Nile\", \"A\", 381229, \"SSD\"]\n",
    "combined_place_df.loc[combined_place_df[\"place_name\"]==\"Gobolka Woqooyi Galbeed\",\"place_name\"] = \"Woqooyi Galbeed\"\n",
    "\n",
    "combined_place_df = combined_place_df[combined_place_df[\"place_name\"]!=\"Pacific County\"]\n",
    "combined_place_df = combined_place_df.loc[combined_place_df[\"word\"]!=\"B.V.\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"MMT\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Yellow\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Hadley\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Western North\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"colonies\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"TN\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"NH\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Mn\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Tx\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"TX\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Tn\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"FL\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Spartina\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Tamarix\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Eurasia\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Phillyrea\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"N-15\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"LT50\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"POSEIDON\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"LC50\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"El Nio\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"La Nia\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Red\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Gulf Stream\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"].str.len()>2]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"NH 1\"]\n",
    "combined_place_df = combined_place_df[combined_place_df[\"word\"]!=\"Quercus\"]\n",
    "\n",
    "\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"ZJP\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"MSW\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"CCS\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"Tier-3\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"N2O\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"VKT\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"OECD\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"States\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"North to South\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"Stabilising\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"Mass Railway\")]\n",
    "combined_place_df = combined_place_df[(combined_place_df[\"word\"]!=\"City\")]\n",
    "\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Ireland\", geocolumns]=[\"PCLI\",53,-8,\"Ireland\",\"A\",2963597,\"IRL\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"United States\", geocolumns] = [\"PCLI\",39.76,-98.5,\"United States\",\"A\",6252001, \"USA\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Czech Republic\", geocolumns] = [\"PCLI\",49.75,15,\"Czechia\",\"A\",3077311, \"CZE\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"Czechia\", geocolumns] = [\"PCLI\",49.75,15,\"Czechia\",\"A\",3077311, \"CZE\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"China\", geocolumns] = [\"PCLI\", 35, 105, \"China\", \"A\", 1814991, \"CHN\"]\n",
    "combined_place_df.loc[combined_place_df[\"word\"]==\"United Arab Emirates\", geocolumns] = [\"PCLI\", 23.75, 54.5, \"United Arab Emirates\", \"A\", 290557, \"ARE\"]\n",
    "\n",
    "\n",
    "# import pycountry_convert as pc\n",
    "# def get_cont(x):\n",
    "#     continents = {\n",
    "#         'NA': 'North America',\n",
    "#         'SA': 'South America', \n",
    "#         'AS': 'Asia',\n",
    "#         'OC': 'Oceania',\n",
    "#         'AF': 'Africa',\n",
    "#         'EU': 'Europe'\n",
    "#     }\n",
    "#     try:\n",
    "#         return continents[pc.country_alpha2_to_continent_code(pc.country_alpha3_to_country_alpha2(x))]\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# combined_place_df['continent'] = combined_place_df['country_code3'].apply(lambda x: get_cont(x))\n",
    "\n",
    "combined_place_df.to_csv('data/places.csv', index=False)\n",
    "\n",
    "print(combined_place_df.shape)\n",
    "\n",
    "combined_place_df.tail()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mordecai-env",
   "language": "python",
   "name": "mordecai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
